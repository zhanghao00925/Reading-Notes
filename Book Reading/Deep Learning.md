# 机器学习基础

### k-均值聚类
一个简单的表示学习方法，将训练集分成k个靠近彼此的不同样本聚类。该算法提供了k维的one-hot编码向量h以表示输入x。

one-hot编码也是一种稀疏表示，丢失了很多分布式表示的有点。one-hot编码仍然有一些统计优点，也具有计算上的优势。

聚类问题本身是病态的，没有单一的标准去度量聚类的数据在真实世界中效果如何。可能有许多不同的聚类都能很好地对应到现实世界的某些属性。我们可能希望找到一个和特征相关的聚类，但是得到了一个和任务无关的，同样是合理的不同聚类。

相对于one-hot表示而言，分布式表示则可以对一个样本赋予多个属性。目前仍然不清楚什么是最优的分布式表示，但是多个属性减少了算法去猜我们关心哪一个属性的负担，允许我们通过比较很多属性而非测试一个单一属性来细粒度地度量相似性。

## 随机梯度下降
随机梯度下降是梯度下降算法的一个扩展。机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的训练集的计算代价也更大。

机器学习算法中的代价函数通常可以分解成每个样本的代价函数的综合。对于这些相加的代价函数，运算的计算代价是O(m)。随机梯度下降的核心是，梯度是期望，可以用小规模的样本近似估计。在算法的每一步，从训练集中均匀抽出以小批量（minibatch）样本。在拟合几十亿的样本时，每次更新计算只用到几百个样本。

随机梯度下降在深度学习之外有很多重要的应用，它是在大规模数据上训练大型线性模型的主要方法。在深度学习兴起之前，学习非线性模型的主要方法是结合核技巧的线性模型。很多核学习算法需要构建一个m*m的矩阵，计算量是O(m^2)。当数据集是几十亿个样本时，这个计算量是不能接受的。

## 构建机器学习算法
几乎所有的深度学习算法都可以被描述为一个相当简单的配方：特定的数据集、代价函数、优化过程和模型。意识到可以替换独立于其他组件的大多数组件，因此我们能得到很多不同的算法。

通常代价函数至少含有一项使学习过程进行统计估计的成分。最常见的代价函数是负对数似然，最小化代价函数导致的最大似然估计。代价函数也可能含有附加项，如正则化项。

如果我们将模型变成非线性的，那么大多数代价函数不再能通过闭解优化，这就要求选择一个迭代数值优化过程。在某些情况下，由于计算原因，不能实际计算代价函数。在这种情况下，只要有近似其梯度的方法，那么我们仍然可以使用迭代数值优化近似最小化目标。

如果一个机器学习算法看上去特别独特或者手动设计的，那么通常需要使用特殊的优化方法进行求解。有些模型，如决策树或k-均值，需要特殊优化，因为它们的代价函数有平坦的区域，使其不适合通过基于梯度的优化去最小化。

## 促使深度学习发展的挑战
促使深度学习发展的一部分原因是传统学习算法在人工智能问题上泛化能力不足。深度学习旨在克服处理高维数据时在新样本上泛化特别困难，以及在传统机器学习中实现泛化机制不适合学习高维空间中复杂的函数。

### 维度灾难
当数据的维数很高时，很多机器学习问题变得相当困难。这种现象被称为维数灾难（curse of dimensionality）。特别值得注意的是，一组变量不同的可能配置数量会随着变量数目的增加而指数级增长。

由维度灾难带来的一个挑战是统计挑战。统计挑战产生于x的可能配置数目远大于训练样本的数目。在高维空间中，参数配置数目远大于样本数目，大部分配置没有相关的样本，许多传统机器学习算法只是简单地假设在一个新点的输出应大致和最接近的训练点的输出相同。

### 局部不变性和平滑正则化
为了更好地泛化，机器学习算法需要由先验信念引导应该学习什么类型的函数。也可以说先验信念直接影响函数本身，而仅仅通过它们对函数的影响来间接改变参数。先验信念还间接地体现在选择一些偏好某类函数的算法。

使用最广泛的隐式“先验”是平滑先验（smooth prior），或局部不变性先验（local constancy prior）。这个先验表明我们学习的函数不应在小区域内发生很大的变化。有许多不同的方法来显式或隐式地表示学习函数应该具有光滑或者局部不变性的先验。

只要在要学习的真实函数的峰值和谷值处有足够的样本，那么平滑性假设和相关的无参数学习算法的效果都非常好。当要学习的函数足够平滑，并且只在少数几维变化时，这样做一般没问题。在高维空间中，即使时非常平滑的函数，也会在不同维度上有不同的变化方式。如果函数在不同的区间中表现不一样，那么就非常难用一组训练样本去刻画函数。

是否可以有效地表示复杂函数以及所估计的函数是否可以很好地泛化到新的输入？答案是有的。关键观点是，只要通过额外假设生产数据分布来建立区间内的依赖关系，那么O(k)个样本足以描述多如O(2^k)的大量区间。通过这种方式，确实能做到非局部泛化。为了利用这些优势，许多不同的深度学习算法都提出的一些适用于多种AI任务的隐式或显式的假设。

一些其他的机器学习方法往往会提出更强的、针对特定问题的假设。通常，神经网络不会包含这些很强的假设，因此可以泛化到更广泛的各种结构中。深度学习的核心思想是假设数据由因素或特征组合产生，这些因素或特征可能来自一个层次结构的多个层级。

### 流形学习
流形（manifold）指连接在一起的区域。数学上，它是指一组点，且每个点都有其邻域。每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位置。机器学习中倾向于更松散地定义一组点，只需要考虑嵌入在高维空间中的自由度或维数就能很好地近似。每一位都对应着局部的变化方向。

流形学习（manifold learning）算法假设认为R^n中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。关键假设是概率质量高度集中。

数据位于低维流形的假设并不总是对的或者有用的，但在一些场景中至少是近似对的。

第一个支持流形假设（manifold hypothesis）的观察是现实生活中的图像、文本的概率分布都是高度集中的。均匀的噪声从来不会与这类邻域的结构化输入类似。集中的概率分布不足以说明数据位于一个相当小的流形中，还必须确保，所遇到的样本和其他样本相互连接，每个样本被其他高度相似的样本包围，而这些高度相似的样本可以通过变换来遍历该流形得到。

第二个论点是，至少能非正式地想象这些邻域和变换。

当数据位于低维流形中时，使用流形中的坐标而非R^n中的坐标表示机器学习数据跟为自然。提取这些流形中的坐标是非常具有挑战性的，但是很有希望改进许多机器学习算法。

# 深度前馈网络
深度前馈网络（deep feedforward network），也叫做前馈神经网络（feedforward neural network）或者多层感知机（multilayer perceptron，MLP），是典型的深度学习模型。被称为前向的（feedforward）的，是因为在模型的输出和模型本身之间没有反馈（feedback）连接。当被扩展成包含反馈连接时，它们被称为循环神经网络（recurrent neural network）。

之所以被称作网络（network），因为它们通常用许多不同函数复合在一起来表示。链式结构是神经网络中最常用的结构。

## 实例：学习XOR
大多数神经网络通过仿射变换之后紧跟着一个被称为激活函数的固定非线性函数来实现用非线性函数描述特征的目标。在现代神经网络中，默认的推荐是使用由激活函数，整流线性单元（rectified linear unit）或者称为ReLU。

## 基于梯度的学习
目前为止看到的线性模型和神经网络的最大区别，在于神经网络的非线性导致大大多数感兴趣的代价函数都变得非凸。这意味着神经网路的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值；而不是像用于训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或SVM的u优化算法那样保证全局收敛。

凸优化从任何一种初始参数出发都会收敛。用于非凸损失函数的随即梯队湘江没有这种收敛性保证，并且对参数的初始值很敏感，将所有的权重值初始化为小随机数是很重要的。

### 代价函数
神经网络的代价函数或多或少是和其他的参数模型相同。大多数情况下，参数模型定义了一个分布p(y|x;theta)并且简单地使用最大似然原理。这意味着使用训练数据和模型预测间的交叉熵作为代价函数。完整的代价函数通常在基本代价函数的基础上结合一个正则项。同于线性模型的权重衰减方法也直接适用于深度神经网络，而且是最流行的正则化策略之一。

#### 使用最大似然学习条件分布
使用最大似然来训练意味着代价函数就是负对数似然，它与训练数据和模型分布间的交叉熵等价。

贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须足够的大和具有足够的预测性，来为学习算法提供一个好的指引。饱和（变得非常平）的函数破坏了这一目标，因为它们把梯度变得非常小。福德对数似然可以帮助在很多模型中避免这个问题。

用于实现对打似然估计的交叉熵代价函数有一个不同寻常的特性，它通常没有最小值。正则化技术提供了一些不同的方法来修正学习问题，使得模型不会获得无限制的收益。

#### 学习条件统计
