# 机器学习基础

### k-均值聚类
一个简单的表示学习方法，将训练集分成k个靠近彼此的不同样本聚类。该算法提供了k维的one-hot编码向量h以表示输入x。

one-hot编码也是一种稀疏表示，丢失了很多分布式表示的有点。one-hot编码仍然有一些统计优点，也具有计算上的优势。

聚类问题本身是病态的，没有单一的标准去度量聚类的数据在真实世界中效果如何。可能有许多不同的聚类都能很好地对应到现实世界的某些属性。我们可能希望找到一个和特征相关的聚类，但是得到了一个和任务无关的，同样是合理的不同聚类。

相对于one-hot表示而言，分布式表示则可以对一个样本赋予多个属性。目前仍然不清楚什么是最优的分布式表示，但是多个属性减少了算法去猜我们关心哪一个属性的负担，允许我们通过比较很多属性而非测试一个单一属性来细粒度地度量相似性。

## 随机梯度下降
随机梯度下降是梯度下降算法的一个扩展。机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的训练集的计算代价也更大。

机器学习算法中的代价函数通常可以分解成每个样本的代价函数的综合。对于这些相加的代价函数，运算的计算代价是O(m)。随机梯度下降的核心是，梯度是期望，可以用小规模的样本近似估计。在算法的每一步，从训练集中均匀抽出以小批量（minibatch）样本。在拟合几十亿的样本时，每次更新计算只用到几百个样本。

随机梯度下降在深度学习之外有很多重要的应用，它是在大规模数据上训练大型线性模型的主要方法。在深度学习兴起之前，学习非线性模型的主要方法是结合核技巧的线性模型。很多核学习算法需要构建一个m*m的矩阵，计算量是O(m^2)。当数据集是几十亿个样本时，这个计算量是不能接受的。

## 构建机器学习算法
几乎所有的深度学习算法都可以被描述为一个相当简单的配方：特定的数据集、代价函数、优化过程和模型。意识到可以替换独立于其他组件的大多数组件，因此我们能得到很多不同的算法。

通常代价函数至少含有一项使学习过程进行统计估计的成分。最常见的代价函数是负对数似然，最小化代价函数导致的最大似然估计。代价函数也可能含有附加项，如正则化项。

如果我们将模型变成非线性的，那么大多数代价函数不再能通过闭解优化，这就要求选择一个迭代数值优化过程。在某些情况下，由于计算原因，不能实际计算代价函数。在这种情况下，只要有近似其梯度的方法，那么我们仍然可以使用迭代数值优化近似最小化目标。

如果一个机器学习算法看上去特别独特或者手动设计的，那么通常需要使用特殊的优化方法进行求解。有些模型，如决策树或k-均值，需要特殊优化，因为它们的代价函数有平坦的区域，使其不适合通过基于梯度的优化去最小化。

## 促使深度学习发展的挑战
促使深度学习发展的一部分原因是传统学习算法在人工智能问题上泛化能力不足。深度学习旨在克服处理高维数据时在新样本上泛化特别困难，以及在传统机器学习中实现泛化机制不适合学习高维空间中复杂的函数。

### 维度灾难
当数据的维数很高时，很多机器学习问题变得相当困难。这种现象被称为维数灾难（curse of dimensionality）。特别值得注意的是，一组变量不同的可能配置数量会随着变量数目的增加而指数级增长。

由维度灾难带来的一个挑战是统计挑战。统计挑战产生于x的可能配置数目远大于训练样本的数目。在高维空间中，参数配置数目远大于样本数目，大部分配置没有相关的样本，许多传统机器学习算法只是简单地假设在一个新点的输出应大致和最接近的训练点的输出相同。

### 局部不变性和平滑正则化
为了更好地泛化，机器学习算法需要由先验信念引导应该学习什么类型的函数。也可以说先验信念直接影响函数本身，而仅仅通过它们对函数的影响来间接改变参数。先验信念还间接地体现在选择一些偏好某类函数的算法。

使用最广泛的隐式“先验”是平滑先验（smooth prior），或局部不变性先验（local constancy prior）。这个先验表明我们学习的函数不应在小区域内发生很大的变化。有许多不同的方法来显式或隐式地表示学习函数应该具有光滑或者局部不变性的先验。

只要在要学习的真实函数的峰值和谷值处有足够的样本，那么平滑性假设和相关的无参数学习算法的效果都非常好。当要学习的函数足够平滑，并且只在少数几维变化时，这样做一般没问题。在高维空间中，即使时非常平滑的函数，也会在不同维度上有不同的变化方式。如果函数在不同的区间中表现不一样，那么就非常难用一组训练样本去刻画函数。

是否可以有效地表示复杂函数以及所估计的函数是否可以很好地泛化到新的输入？答案是有的。关键观点是，只要通过额外假设生产数据分布来建立区间内的依赖关系，那么O(k)个样本足以描述多如O(2^k)的大量区间。通过这种方式，确实能做到非局部泛化。为了利用这些优势，许多不同的深度学习算法都提出的一些适用于多种AI任务的隐式或显式的假设。

一些其他的机器学习方法往往会提出更强的、针对特定问题的假设。通常，神经网络不会包含这些很强的假设，因此可以泛化到更广泛的各种结构中。深度学习的核心思想是假设数据由因素或特征组合产生，这些因素或特征可能来自一个层次结构的多个层级。

### 流形学习
流形（manifold）指连接在一起的区域。数学上，它是指一组点，且每个点都有其邻域。每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位置。机器学习中倾向于更松散地定义一组点，只需要考虑嵌入在高维空间中的自由度或维数就能很好地近似。每一位都对应着局部的变化方向。

流形学习（manifold learning）算法假设认为R^n中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。关键假设是概率质量高度集中。

数据位于低维流形的假设并不总是对的或者有用的，但在一些场景中至少是近似对的。

第一个支持流形假设（manifold hypothesis）的观察是现实生活中的图像、文本的概率分布都是高度集中的。均匀的噪声从来不会与这类邻域的结构化输入类似。集中的概率分布不足以说明数据位于一个相当小的流形中，还必须确保，所遇到的样本和其他样本相互连接，每个样本被其他高度相似的样本包围，而这些高度相似的样本可以通过变换来遍历该流形得到。

第二个论点是，至少能非正式地想象这些邻域和变换。

当数据位于低维流形中时，使用流形中的坐标而非R^n中的坐标表示机器学习数据跟为自然。提取这些流形中的坐标是非常具有挑战性的，但是很有希望改进许多机器学习算法。
