# 深度前馈网络
深度前馈网络（deep feedforward network），也叫做前馈神经网络（feedforward neural network）或者多层感知机（multilayer perceptron，MLP），是典型的深度学习模型。被称为前向的（feedforward）的，是因为在模型的输出和模型本身之间没有反馈（feedback）连接。当被扩展成包含反馈连接时，它们被称为循环神经网络（recurrent neural network）。

之所以被称作网络（network），因为它们通常用许多不同函数复合在一起来表示。链式结构是神经网络中最常用的结构。

## 实例：学习XOR
大多数神经网络通过仿射变换之后紧跟着一个被称为激活函数的固定非线性函数来实现用非线性函数描述特征的目标。在现代神经网络中，默认的推荐是使用由激活函数，整流线性单元（rectified linear unit）或者称为ReLU。

## 基于梯度的学习
目前为止看到的线性模型和神经网络的最大区别，在于神经网络的非线性导致大大多数感兴趣的代价函数都变得非凸。这意味着神经网路的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值；而不是像用于训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或SVM的u优化算法那样保证全局收敛。

凸优化从任何一种初始参数出发都会收敛。用于非凸损失函数的随即梯队湘江没有这种收敛性保证，并且对参数的初始值很敏感，将所有的权重值初始化为小随机数是很重要的。

### 代价函数
神经网络的代价函数或多或少是和其他的参数模型相同。大多数情况下，参数模型定义了一个分布p(y|x;theta)并且简单地使用最大似然原理。这意味着使用训练数据和模型预测间的交叉熵作为代价函数。完整的代价函数通常在基本代价函数的基础上结合一个正则项。同于线性模型的权重衰减方法也直接适用于深度神经网络，而且是最流行的正则化策略之一。

#### 使用最大似然学习条件分布
使用最大似然来训练意味着代价函数就是负对数似然，它与训练数据和模型分布间的交叉熵等价。

贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须足够的大和具有足够的预测性，来为学习算法提供一个好的指引。饱和（变得非常平）的函数破坏了这一目标，因为它们把梯度变得非常小。福德对数似然可以帮助在很多模型中避免这个问题。

用于实现对打似然估计的交叉熵代价函数有一个不同寻常的特性，它通常没有最小值。正则化技术提供了一些不同的方法来修正学习问题，使得模型不会获得无限制的收益。

#### 学习条件统计