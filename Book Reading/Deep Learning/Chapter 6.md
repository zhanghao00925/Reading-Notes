# 深度前馈网络
深度前馈网络（deep feedforward network），也叫做前馈神经网络（feedforward neural network）或者多层感知机（multilayer perceptron，MLP），是典型的深度学习模型。被称为前向的（feedforward）的，是因为在模型的输出和模型本身之间没有反馈（feedback）连接。当被扩展成包含反馈连接时，它们被称为循环神经网络（recurrent neural network）。

之所以被称作网络（network），因为它们通常用许多不同函数复合在一起来表示。链式结构是神经网络中最常用的结构。

## 实例：学习XOR
大多数神经网络通过仿射变换之后紧跟着一个被称为激活函数的固定非线性函数来实现用非线性函数描述特征的目标。在现代神经网络中，默认的推荐是使用由激活函数，整流线性单元（rectified linear unit）或者称为ReLU。

## 基于梯度的学习
目前为止看到的线性模型和神经网络的最大区别，在于神经网络的非线性导致大大多数感兴趣的代价函数都变得非凸。这意味着神经网路的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值；而不是像用于训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或SVM的u优化算法那样保证全局收敛。

凸优化从任何一种初始参数出发都会收敛。用于非凸损失函数的随即梯队湘江没有这种收敛性保证，并且对参数的初始值很敏感，将所有的权重值初始化为小随机数是很重要的。

### 代价函数
神经网络的代价函数或多或少是和其他的参数模型相同。大多数情况下，参数模型定义了一个分布p(y|x;theta)并且简单地使用最大似然原理。这意味着使用训练数据和模型预测间的交叉熵作为代价函数。完整的代价函数通常在基本代价函数的基础上结合一个正则项。同于线性模型的权重衰减方法也直接适用于深度神经网络，而且是最流行的正则化策略之一。

#### 使用最大似然学习条件分布
使用最大似然来训练意味着代价函数就是负对数似然，它与训练数据和模型分布间的交叉熵等价。

贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须足够的大和具有足够的预测性，来为学习算法提供一个好的指引。饱和（变得非常平）的函数破坏了这一目标，因为它们把梯度变得非常小。福德对数似然可以帮助在很多模型中避免这个问题。

用于实现对打似然估计的交叉熵代价函数有一个不同寻常的特性，它通常没有最小值。正则化技术提供了一些不同的方法来修正学习问题，使得模型不会获得无限制的收益。

#### 学习条件统计

有时候我们并不是想学习一个完整的概率分布$p(y|x;\theta)$，而仅仅是想学习在给定$x$时$y$的某个条件统计量。

我们可以把代价函数看作一个泛函，而不仅仅是一个函数。泛函是函数到实数的映射。因此，我们可以将学习看作选择一个函数，而不仅仅是选择一组参数。对函数求解优化问题需要用到变分法这个数学工具。

可惜的是，均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。这就是交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了，即使是在没必要估计整个$p(y|x)$分布时。

### 输出单元

代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式。

#### 用于高斯输出分布的线性单元

一种简单的输出单元是基于仿射变换的输出单元，这些单元往往被直接称为线性单元。线性输出层经常被用来产生条件高斯分布的均值。

最大似然框架也使得学习高斯分布的协方差矩阵更加容易，或者更加容易地使高斯分布的协方差矩阵作为输入的函数。然而，对于所有输入，协方差矩阵都必须被限定成一个正定矩阵。线性输出层很难满足着这种限定，所以通常使用其他的输出单元来对协方差参数化。

因为线性单元不会饱和，所以它们已于采用基于梯度的优化算法，甚至可以使用其他多种优化算法。

#### 用于Bernoulli输出分布的sigmoid单元

许多任务需要预测二值型变量$y$的值。具有两个类的分类问题可以归结为这种形式。

有效的概率必须处在区间$[0,1]$之间，为满足该约束条件需要一些细致的设计工作。最好是使用一种新的方法来保证无论何时模型给出了错误的答案时，总能有一个较大的梯度。这种方法是基于使用sigmoid输出单元结合最大似然来实现的。

基于指数和归一化的概率分布在统计建模的文献中很常见。用于定义这种二值型变量分布的变量$z$被称为分对数。

#### 用于Multinoulli输出分布的softmax单元

任何时候，当我们想要表示一个具有$n$个可能取值的离散型随机变量的分布时，都可以使用softmax函数。他可以看作sigmoid函数的扩展，其中sigmoid函数用来表示二值型变量的分布。

